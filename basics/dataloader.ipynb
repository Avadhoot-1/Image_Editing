{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8-LbGgH8uMC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAg_qvN4FQb6"
      },
      "source": [
        "# Transfer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZhpV65S9QFc",
        "outputId": "32ba2a1a-5ee0-4810-f178-615dfefd8b2a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
            "              ReLU-2         [-1, 64, 224, 224]               0\n",
            "         MaxPool2d-3         [-1, 64, 112, 112]               0\n",
            "            Conv2d-4        [-1, 128, 112, 112]          73,856\n",
            "              ReLU-5        [-1, 128, 112, 112]               0\n",
            "         MaxPool2d-6          [-1, 128, 56, 56]               0\n",
            "            Conv2d-7          [-1, 256, 56, 56]         295,168\n",
            "              ReLU-8          [-1, 256, 56, 56]               0\n",
            "            Conv2d-9          [-1, 256, 56, 56]         590,080\n",
            "             ReLU-10          [-1, 256, 56, 56]               0\n",
            "        MaxPool2d-11          [-1, 256, 28, 28]               0\n",
            "           Conv2d-12          [-1, 512, 28, 28]       1,180,160\n",
            "             ReLU-13          [-1, 512, 28, 28]               0\n",
            "           Conv2d-14          [-1, 512, 28, 28]       2,359,808\n",
            "             ReLU-15          [-1, 512, 28, 28]               0\n",
            "        MaxPool2d-16          [-1, 512, 14, 14]               0\n",
            "           Conv2d-17          [-1, 512, 14, 14]       2,359,808\n",
            "             ReLU-18          [-1, 512, 14, 14]               0\n",
            "           Conv2d-19          [-1, 512, 14, 14]       2,359,808\n",
            "             ReLU-20          [-1, 512, 14, 14]               0\n",
            "        MaxPool2d-21            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-22            [-1, 512, 7, 7]               0\n",
            "           Linear-23                 [-1, 4096]     102,764,544\n",
            "             ReLU-24                 [-1, 4096]               0\n",
            "          Dropout-25                 [-1, 4096]               0\n",
            "           Linear-26                 [-1, 4096]      16,781,312\n",
            "             ReLU-27                 [-1, 4096]               0\n",
            "          Dropout-28                 [-1, 4096]               0\n",
            "           Linear-29                 [-1, 1000]       4,097,000\n",
            "================================================================\n",
            "Total params: 132,863,336\n",
            "Trainable params: 0\n",
            "Non-trainable params: 132,863,336\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 125.37\n",
            "Params size (MB): 506.83\n",
            "Estimated Total Size (MB): 632.78\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "pre_model = None\n",
        "pre_model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', pretrained=True)\n",
        "# pre_model = pre_model.cuda()\n",
        "for param in pre_model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "summary(pre_model, (3,224,224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyjkjow59bs8",
        "outputId": "30ebc4dd-c693-47e4-94e0-0753fb312525"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "features.0.weight 1728\n",
            "features.0.bias 64\n",
            "features.3.weight 73728\n",
            "features.3.bias 128\n",
            "features.6.weight 294912\n",
            "features.6.bias 256\n",
            "features.8.weight 589824\n",
            "features.8.bias 256\n",
            "features.11.weight 1179648\n",
            "features.11.bias 512\n",
            "features.13.weight 2359296\n",
            "features.13.bias 512\n",
            "features.16.weight 2359296\n",
            "features.16.bias 512\n",
            "features.18.weight 2359296\n",
            "features.18.bias 512\n",
            "classifier.0.weight 102760448\n",
            "classifier.0.bias 4096\n",
            "classifier.3.weight 16777216\n",
            "classifier.3.bias 4096\n",
            "classifier.6.weight 4096000\n",
            "classifier.6.bias 1000\n"
          ]
        }
      ],
      "source": [
        "for title, param in pre_model.named_parameters():\n",
        "  print(title, param.numel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B269L8QL_M1B"
      },
      "outputs": [],
      "source": [
        "for title, param in pre_model.named_parameters():\n",
        "  if title.startswith('features.18'):\n",
        "    param.requires_grad = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6367KVDA7Ya"
      },
      "outputs": [],
      "source": [
        "class New_model(nn.Module):\n",
        "\n",
        "  def __init__(self, pre_model, classes):\n",
        "    super().__init__()\n",
        "\n",
        "    self.pre = pre_model\n",
        "    self.pre.classifier = nn.Sequential(\n",
        "        nn.Linear(512*7*7, 128),\n",
        "        nn.Linear(128, classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pre(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZtDUqZv85hb"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"Using GPU\")\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9KVJKvaCGPL",
        "outputId": "8c760bb1-a075-4f58-aa7a-6fead69c31a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
            "              ReLU-2         [-1, 64, 224, 224]               0\n",
            "         MaxPool2d-3         [-1, 64, 112, 112]               0\n",
            "            Conv2d-4        [-1, 128, 112, 112]          73,856\n",
            "              ReLU-5        [-1, 128, 112, 112]               0\n",
            "         MaxPool2d-6          [-1, 128, 56, 56]               0\n",
            "            Conv2d-7          [-1, 256, 56, 56]         295,168\n",
            "              ReLU-8          [-1, 256, 56, 56]               0\n",
            "            Conv2d-9          [-1, 256, 56, 56]         590,080\n",
            "             ReLU-10          [-1, 256, 56, 56]               0\n",
            "        MaxPool2d-11          [-1, 256, 28, 28]               0\n",
            "           Conv2d-12          [-1, 512, 28, 28]       1,180,160\n",
            "             ReLU-13          [-1, 512, 28, 28]               0\n",
            "           Conv2d-14          [-1, 512, 28, 28]       2,359,808\n",
            "             ReLU-15          [-1, 512, 28, 28]               0\n",
            "        MaxPool2d-16          [-1, 512, 14, 14]               0\n",
            "           Conv2d-17          [-1, 512, 14, 14]       2,359,808\n",
            "             ReLU-18          [-1, 512, 14, 14]               0\n",
            "           Conv2d-19          [-1, 512, 14, 14]       2,359,808\n",
            "             ReLU-20          [-1, 512, 14, 14]               0\n",
            "        MaxPool2d-21            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-22            [-1, 512, 7, 7]               0\n",
            "           Linear-23                  [-1, 128]       3,211,392\n",
            "           Linear-24                  [-1, 200]          25,800\n",
            "              VGG-25                  [-1, 200]               0\n",
            "================================================================\n",
            "Total params: 12,457,672\n",
            "Trainable params: 5,597,000\n",
            "Non-trainable params: 6,860,672\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 125.18\n",
            "Params size (MB): 47.52\n",
            "Estimated Total Size (MB): 173.28\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "model = New_model(pre_model, 200)\n",
        "model.to(device)\n",
        "summary(model, (3,224,224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fSXA2adDDGW",
        "outputId": "3ec8a0cb-ee85-4f73-b73e-d5d3a58f41db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pre.features.18.weight 2359296\n",
            "pre.features.18.bias 512\n",
            "pre.classifier.0.weight 3211264\n",
            "pre.classifier.0.bias 128\n",
            "pre.classifier.1.weight 25600\n",
            "pre.classifier.1.bias 200\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for title, param in model.named_parameters():\n",
        "  if(param.requires_grad):\n",
        "    print(title, param.numel())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNhfPrl2FVzy"
      },
      "source": [
        "# DataSet Preparation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qi-YcnslFeal",
        "outputId": "3bbc11d9-788c-47e2-f3ea-80b51b8497f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'IMagenet' already exists and is not an empty directory.\n",
            "test  train  val  wnids.txt  words.txt\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/seshuad/IMagenet\n",
        "! ls 'IMagenet/tiny-imagenet-200/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9alnIGFvHq5-",
        "outputId": "a82c1782-b6a6-484a-f544-8b71507a6d1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n02124075': 0, 'n04067472': 1, 'n04540053': 2, 'n04099969': 3, 'n07749582': 4, 'n01641577': 5, 'n02802426': 6, 'n09246464': 7, 'n07920052': 8, 'n03970156': 9, 'n03891332': 10, 'n02106662': 11, 'n03201208': 12, 'n02279972': 13, 'n02132136': 14, 'n04146614': 15, 'n07873807': 16, 'n02364673': 17, 'n04507155': 18, 'n03854065': 19, 'n03838899': 20, 'n03733131': 21, 'n01443537': 22, 'n07875152': 23, 'n03544143': 24, 'n09428293': 25, 'n03085013': 26, 'n02437312': 27, 'n07614500': 28, 'n03804744': 29, 'n04265275': 30, 'n02963159': 31, 'n02486410': 32, 'n01944390': 33, 'n09256479': 34, 'n02058221': 35, 'n04275548': 36, 'n02321529': 37, 'n02769748': 38, 'n02099712': 39, 'n07695742': 40, 'n02056570': 41, 'n02281406': 42, 'n01774750': 43, 'n02509815': 44, 'n03983396': 45, 'n07753592': 46, 'n04254777': 47, 'n02233338': 48, 'n04008634': 49, 'n02823428': 50, 'n02236044': 51, 'n03393912': 52, 'n07583066': 53, 'n04074963': 54, 'n01629819': 55, 'n09332890': 56, 'n02481823': 57, 'n03902125': 58, 'n03404251': 59, 'n09193705': 60, 'n03637318': 61, 'n04456115': 62, 'n02666196': 63, 'n03796401': 64, 'n02795169': 65, 'n02123045': 66, 'n01855672': 67, 'n01882714': 68, 'n02917067': 69, 'n02988304': 70, 'n04398044': 71, 'n02843684': 72, 'n02423022': 73, 'n02669723': 74, 'n04465501': 75, 'n02165456': 76, 'n03770439': 77, 'n02099601': 78, 'n04486054': 79, 'n02950826': 80, 'n03814639': 81, 'n04259630': 82, 'n03424325': 83, 'n02948072': 84, 'n03179701': 85, 'n03400231': 86, 'n02206856': 87, 'n03160309': 88, 'n01984695': 89, 'n03977966': 90, 'n03584254': 91, 'n04023962': 92, 'n02814860': 93, 'n01910747': 94, 'n04596742': 95, 'n03992509': 96, 'n04133789': 97, 'n03937543': 98, 'n02927161': 99, 'n01945685': 100, 'n02395406': 101, 'n02125311': 102, 'n03126707': 103, 'n04532106': 104, 'n02268443': 105, 'n02977058': 106, 'n07734744': 107, 'n03599486': 108, 'n04562935': 109, 'n03014705': 110, 'n04251144': 111, 'n04356056': 112, 'n02190166': 113, 'n03670208': 114, 'n02002724': 115, 'n02074367': 116, 'n04285008': 117, 'n04560804': 118, 'n04366367': 119, 'n02403003': 120, 'n07615774': 121, 'n04501370': 122, 'n03026506': 123, 'n02906734': 124, 'n01770393': 125, 'n04597913': 126, 'n03930313': 127, 'n04118538': 128, 'n04179913': 129, 'n04311004': 130, 'n02123394': 131, 'n04070727': 132, 'n02793495': 133, 'n02730930': 134, 'n02094433': 135, 'n04371430': 136, 'n04328186': 137, 'n03649909': 138, 'n04417672': 139, 'n03388043': 140, 'n01774384': 141, 'n02837789': 142, 'n07579787': 143, 'n04399382': 144, 'n02791270': 145, 'n03089624': 146, 'n02814533': 147, 'n04149813': 148, 'n07747607': 149, 'n03355925': 150, 'n01983481': 151, 'n04487081': 152, 'n03250847': 153, 'n03255030': 154, 'n02892201': 155, 'n02883205': 156, 'n03100240': 157, 'n02415577': 158, 'n02480495': 159, 'n01698640': 160, 'n01784675': 161, 'n04376876': 162, 'n03444034': 163, 'n01917289': 164, 'n01950731': 165, 'n03042490': 166, 'n07711569': 167, 'n04532670': 168, 'n03763968': 169, 'n07768694': 170, 'n02999410': 171, 'n03617480': 172, 'n06596364': 173, 'n01768244': 174, 'n02410509': 175, 'n03976657': 176, 'n01742172': 177, 'n03980874': 178, 'n02808440': 179, 'n02226429': 180, 'n02231487': 181, 'n02085620': 182, 'n01644900': 183, 'n02129165': 184, 'n02699494': 185, 'n03837869': 186, 'n02815834': 187, 'n07720875': 188, 'n02788148': 189, 'n02909870': 190, 'n03706229': 191, 'n07871810': 192, 'n03447447': 193, 'n02113799': 194, 'n12267677': 195, 'n03662601': 196, 'n02841315': 197, 'n07715103': 198, 'n02504458': 199}\n"
          ]
        }
      ],
      "source": [
        "classes = open('IMagenet/tiny-imagenet-200/wnids.txt').read().splitlines()\n",
        "class_id = {classes[i]:i for i in range(len(classes))}\n",
        "print(class_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lja1bcZJN_f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, root_dir, class_id, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.class_id_dict = class_id\n",
        "        self.transform = transform\n",
        "        self.class_folders = [f for f in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, f))]\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        for class_folder in self.class_folders:\n",
        "          img_paths = glob.glob(os.path.join(root_dir, class_folder,'images' ,'*.JPEG'))\n",
        "          self.image_paths.extend(img_paths)\n",
        "          self.labels.extend([class_folder] * len(img_paths))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.class_id_dict[self.labels[idx]]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "class ValDataset(Dataset):\n",
        "    def __init__(self, root_dir, class_id, image_class_dict, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.class_id_dict = class_id\n",
        "        self.image_class = image_class_dict\n",
        "        self.transform = transform\n",
        "        self.image_paths = glob.glob(os.path.join(root_dir,'images' ,'*.JPEG'))\n",
        "        self.labels = []\n",
        "\n",
        "        for img in self.image_paths:\n",
        "          img_name = img.split('/')[-1]\n",
        "          self.labels.append(img_name)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.class_id_dict[self.image_class[self.labels[idx]]]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSWMuoJEXhaE"
      },
      "outputs": [],
      "source": [
        "img_desc = open('IMagenet/tiny-imagenet-200/val/val_annotations.txt').read().splitlines()\n",
        "image_class = {i.split('\\t')[0]: i.split('\\t')[1] for i in img_desc}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzesSlFyM8xr"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.Resize(244), transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "                               ])\n",
        "train_dataset = TrainDataset('IMagenet/tiny-imagenet-200/train/', class_id, transform)\n",
        "val_dataset = ValDataset('IMagenet/tiny-imagenet-200/val/', class_id,image_class, transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size = 256, shuffle = True)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 256, shuffle = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI_11GrSpP7P"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-d1lb42n7lr"
      },
      "outputs": [],
      "source": [
        "\n",
        "num_epochs = 5\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZOTX_uzqLup",
        "outputId": "e953bb32-c183-4517-93e3-ce660babf0c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  19%|█▊        | 73/391 [2:28:39<10:48:54, 122.44s/it]"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "train_losses, test_losses = [], []\n",
        "to_print = True\n",
        "for e in range(num_epochs):\n",
        "  running_loss = 0\n",
        "  for images, labels in tqdm(train_loader, desc =\"Training\"):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    labels = labels.to(device)\n",
        "    images = images.to(device)\n",
        "    output = model(images)\n",
        "\n",
        "    loss = criterion(output, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "  else:\n",
        "    test_loss = 0\n",
        "    accuracy = 0\n",
        "    with torch.no_grad():\n",
        "      model.eval()\n",
        "\n",
        "      # Validation pass\n",
        "      for images, labels in val_loader:\n",
        "\n",
        "        labels = labels.to(device)\n",
        "        images = images.to(device)\n",
        "        log_ps = model(images)\n",
        "\n",
        "        test_loss += criterion(log_ps, labels)\n",
        "\n",
        "        ps = torch.exp(log_ps)\n",
        "        top_p, top_class = ps.topk(1, dim = 1)\n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "    model.train()\n",
        "    train_losses.append(running_loss/len(train_loader))\n",
        "    test_losses.append(test_loss/len(val_loader))\n",
        "\n",
        "    print(\"Epoch: {}/{}..\".format(e+1, num_epochs),\n",
        "          \"Training loss: {:.3f}..\".format(running_loss/len(train_loader)),\n",
        "          \"Test loss: {:.3f}..\".format(test_loss/len(val_loader)),\n",
        "          \"Test Accuracy: {:.3f}\".format(accuracy/len(val_loader)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}